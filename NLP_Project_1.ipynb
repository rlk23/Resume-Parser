{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prelude"
      ],
      "metadata": {
        "id": "EzX92f4BxzPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install poppler-utils\n",
        "!apt-get install tesseract-ocr\n",
        "!pip install pytesseract\n",
        "!pip install pdf2image\n",
        "!pip install docx2txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IN3xdSyP2DIt",
        "outputId": "1a07962f-0013-4887-be86-4beb561a371e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "poppler-utils is already the newest version (0.86.1-0ubuntu1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2build2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.10)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (8.4.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (1.16.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (8.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: docx2txt in /usr/local/lib/python3.10/dist-packages (0.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOXWwoF4WCBx",
        "outputId": "bdf1111c-9c5b-46ce-8cc1-34481e71f51c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-03 21:33:13.884232: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.5.0) (3.5.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "j9kKDTJP6_H7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below extracts from both word documets and pdf documents and returns a text of it for further analysis."
      ],
      "metadata": {
        "id": "V5dG_DvuP0hL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SytvqQRNSMC4"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import docx2txt\n",
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "import spacy\n",
        "import docx2txt\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "\n",
        "def extract_text_from_doc(doc_path):\n",
        "    if doc_path.lower().endswith('.docx'):\n",
        "        temp = docx2txt.process(doc_path)\n",
        "    elif doc_path.lower().endswith('.pdf'):\n",
        "        temp = extract_text_from_scanned_pdf(doc_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please provide a PDF or DOCX file.\")\n",
        "    \n",
        "    text = [line.replace('\\t', ' ') for line in temp.split('\\n') if line]\n",
        "    return ' '.join(text)\n",
        "\n",
        "def extract_text_from_scanned_pdf(pdf_path):\n",
        "    images = convert_from_path(pdf_path)\n",
        "    temp = ''\n",
        "    for img in images:\n",
        "        img_text = pytesseract.image_to_string(img)\n",
        "        temp += img_text + '\\n'\n",
        "\n",
        "    text = [line.replace('\\t', ' ') for line in temp.split('\\n') if line]\n",
        "    return ' '.join(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Named Entity Recognition (NER)"
      ],
      "metadata": {
        "id": "NGmNluMw7F1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the extracted text we extract the name from each resume"
      ],
      "metadata": {
        "id": "dSHB-atMQtVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "# load pre-trained model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def extract_name(resume_text):\n",
        "    nlp_text = nlp(resume_text)\n",
        "    \n",
        "    # define the pattern\n",
        "    pattern = [{'POS': 'PROPN', 'OP': '+'}]\n",
        "    \n",
        "    # initialize matcher with the pattern\n",
        "    matcher = Matcher(nlp.vocab)\n",
        "    matcher.add('NAME', [pattern])\n",
        "    \n",
        "    # get matches from the matcher\n",
        "    matches = matcher(nlp_text)\n",
        "    \n",
        "    # iterate over the matches and extract the span\n",
        "    for match_id, start, end in matches:\n",
        "        span = nlp_text[start:end]\n",
        "        # Ensure that the matched tokens are consecutive proper nouns\n",
        "        if all(token.pos_ == \"PROPN\" for token in span):\n",
        "            return span.text\n"
      ],
      "metadata": {
        "id": "R4yhwOFpfYRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the extracted text we extract the phone number from each resume."
      ],
      "metadata": {
        "id": "bTiNNSd_Q9Jp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_mobile_number(text):\n",
        "    phone = re.findall(re.compile(r'(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?'), text)\n",
        "    \n",
        "    if phone:\n",
        "        number = ''.join(phone[0])\n",
        "        if len(number) > 10:\n",
        "            return '+' + number\n",
        "        else:\n",
        "            return number"
      ],
      "metadata": {
        "id": "zAJFCiixXMmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the extracted text we use regex to extract the emails from each text."
      ],
      "metadata": {
        "id": "5AxXYONKRDYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_email(email):\n",
        "    email = re.findall(\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", email)\n",
        "    if email:\n",
        "        try:\n",
        "            return email[0].split()[0].strip(';')\n",
        "        except IndexError:\n",
        "            return None"
      ],
      "metadata": {
        "id": "EOXPYrfGXO5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the extracted text we extracted the skills."
      ],
      "metadata": {
        "id": "3qPXCuLlRchd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "SKILLS_DB = [\n",
        "'machine learning','data science','python','word','excel','java','javascript','sql','ruby','php','swift','objective-c','typescript','c++','go','rust','scala','haskell','html',\n",
        "'css','react','angular','vue','node.js','express','django','flask','ruby','mysql','postgresql','oracle','mongodb','redis','cassandra',\n",
        "'aws','azure','docker','scrum','git','funtional testing', 'algorithms and data structures','object-oriented programming','software architecture','rest apis'\n",
        "]\n",
        " \n",
        "\n",
        "\n",
        "def extract_skills(input_text):\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    word_tokens = nltk.tokenize.word_tokenize(input_text)\n",
        " \n",
        "    # remove the stop words\n",
        "    filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
        " \n",
        "    # remove the punctuation\n",
        "    filtered_tokens = [w for w in word_tokens if w.isalpha()]\n",
        " \n",
        "    # generate bigrams and trigrams (such as artificial intelligence)\n",
        "    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\n",
        " \n",
        "    # we create a set to keep the results in.\n",
        "    found_skills = set()\n",
        " \n",
        "    # we search for each token in our skills database\n",
        "    for token in filtered_tokens:\n",
        "        if token.lower() in SKILLS_DB:\n",
        "            found_skills.add(token)\n",
        " \n",
        "    # we search for each bigram and trigram in our skills database\n",
        "    for ngram in bigrams_trigrams:\n",
        "        if ngram.lower() in SKILLS_DB:\n",
        "            found_skills.add(ngram)\n",
        " \n",
        "    return found_skills"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjtXaESKyVrZ",
        "outputId": "6a1f261b-c93d-4c3c-de07-edf1b92fdbe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the extracted text we extracted education."
      ],
      "metadata": {
        "id": "F-gu-HOjR1gs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        " \n",
        " \n",
        "RESERVED_WORDS = [\n",
        "    'school',\n",
        "    'college',\n",
        "    'univers',\n",
        "    'academy',\n",
        "    'faculty',\n",
        "    'institute',\n",
        "    'faculdades',\n",
        "    'Schola',\n",
        "    'schule',\n",
        "    'lise',\n",
        "    'lyceum',\n",
        "    'lycee',\n",
        "    'polytechnic',\n",
        "    'kolej',\n",
        "    'ünivers',\n",
        "    'okul',\n",
        "]\n",
        " \n",
        "\n",
        " \n",
        " \n",
        "def extract_education(input_text):\n",
        "    organizations = []\n",
        " \n",
        "    # first get all the organization names using nltk\n",
        "    for sent in nltk.sent_tokenize(input_text):\n",
        "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
        "            if hasattr(chunk, 'label') and chunk.label() == 'ORGANIZATION':\n",
        "                organizations.append(' '.join(c[0] for c in chunk.leaves()))\n",
        " \n",
        "    # we search for each bigram and trigram for reserved words\n",
        "    # (college, university etc...)\n",
        "    education = set()\n",
        "    for org in organizations:\n",
        "        for word in RESERVED_WORDS:\n",
        "            if org.lower().find(word) >= 0:\n",
        "                education.add(org)\n",
        " \n",
        "    return education"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sV1HVWF2YDbN",
        "outputId": "d671599f-47ed-45aa-e3f0-9c5f07936e3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Have a Try"
      ],
      "metadata": {
        "id": "4tWE1rOF7fMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjQmm9c7VXrZ",
        "outputId": "ee7d4d28-30a4-48f6-f45a-755cbf0eecc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data  =extract_text_from_doc(\"/content/Rami-Lionel-Kuttab-Resume_SuggestedEdits.docx\")\n",
        "resume_name = \"John Doe.docx\"\n"
      ],
      "metadata": {
        "id": "j6eo-jumaFoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "description = extract_text_from_doc(\"/content/About the job.docx\")"
      ],
      "metadata": {
        "id": "RJLDd6jU411z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IXXEyKapdnn",
        "outputId": "64b25122-f772-4e40-aefd-482c96bd2a25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rami-Lionel Kuttab (917) 9945360 • rlk_23@outlook.com • linkedin.com/in/rami-kuttab/ EDUCATION Syracuse University, School of Engineering and Computer Science, Syracuse, NY   August 2022 – May 2024       M.S. in Computer Science Relevant Coursework: Operating Systems, Algorithms, Data Science, Computer Architecture, Structured Programming, Natural Language Processing Syracuse University, School of Engineering and Computer Science, Syracuse, NY                                      August 2018 – May 2022                                                                                                                B.S. in Computer Science Relevant Coursework: Data Structures, Algorithms, Agile Development, Database, Object Oriented in C++, Introduction into A.I., Operating Systems, Machine Learning, and Physics WORK EXPERIENCE Software Developer, BGC Partners, New York, New York                     June 2022 – August 2022 Developed and thoroughly tested the code, continuously refining and rewriting it as necessary. Collaborated with other programmers involved in the project, ultimately optimizing the code for a 25 % reduction in running time by transitioning from series to parallel implementation. Researched, designed, and implemented three new software programs that added functionalities to the trading system. Using Python and Pandas Library I built software for my colleagues to assist them in figuring out when the versions have been updated by automatically sending out an email. Participated in +50 code reviews to improve code quality that increased performance by 12% which promoted professional development. PROJECTS WhatsApp Clone – Full-Stack Developer                         July 2021 – August 2021  Developed full-stack clone for WhatsApp that enables users to send text messages to each other in real time. Designed and implemented a smooth user interface using React Native components and styling, while ensuring the app’s compatibility with multiple devices. Utilized Firebase’s real-time database authentication services for back-end, including secure data storage and messaging services for real-time chat. College Events – Front-End Developer                                                                  February 2021 – June 2021  Built, assessed, and produced a website using HTML, CSS, and JavaScript to produce an event organizing website for college students that had an average daily visitor of 60 college students. Utilized UML diagrams to develop a user-friendly interface using HTML and CSS with Bootstrap Led a team of three college students and followed agile development scrums with colleagues to ensure timely completion and release of the project.  Spotify Recommendation System – Data Scientist                                                                   August 2022 – December 2022  Used Collaborative Filtering to recommend songs based on the overlap of songs in playlists in the dataset. Utilized Spotify Web API to fetch data from the Spotify music catalog and manage user’s playlists and saved music. Implemented my own playlist and compared it to Spotify Web API to get recommendations on the top 30 related songs from the Web Spotify API to the current playlist created. TECHNICAL SKILLS Programming Languages: Python, R, SQL, Java, HTML, Oracle, C, C++, PHP, CSS, JavaScript, Ruby Database Management: SQL, PostgreSQL, SQLite Software: MS Excel, MS Word, MS PowerPoint, Visual Studios LEADERSHIP EXPERIENCE Campaign Leader, National Social Service Scheme, Beit Hanina, Israel                                                   Summer 2019 Administered garbage segregation implementation program with 15 volunteers for a village to create awareness. Communicated with the trash company to collect solid garbage waste from the rural parts of Beit Hanina. Devised a wet waste management process to generate organic manure with zero cost of production.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name =  extract_name(data)"
      ],
      "metadata": {
        "id": "QRoCpfhWdW06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiWefpXYgu6_",
        "outputId": "7fb43458-9513-4851-f434-de3b84407b61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rami\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mobile = extract_mobile_number(data)"
      ],
      "metadata": {
        "id": "5DaTf0hSd-F6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mobile)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQvFHkc9hclD",
        "outputId": "010f8860-92df-4829-bf4b-0c7fe68045a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9179945360\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "email = extract_email(data)"
      ],
      "metadata": {
        "id": "G5OiehM6hiRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(email)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULXr5K_dhofO",
        "outputId": "000a5155-6a5a-4e50-ffab-73cd36b46043"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rlk_23@outlook.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "skills = extract_skills(data)"
      ],
      "metadata": {
        "id": "S42FKc5ih8aZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(skills)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmaYoSXbkPzd",
        "outputId": "7cafe6b9-ae16-40bf-e537-31285f1e49dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Java', 'SQL', 'Ruby', 'CSS', 'Word', 'Excel', 'Oracle', 'HTML', 'Data Science', 'PHP', 'React', 'Python', 'PostgreSQL', 'Machine Learning', 'JavaScript'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "skills = list(skills)"
      ],
      "metadata": {
        "id": "53rWnvj67a28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "education = extract_education(data)"
      ],
      "metadata": {
        "id": "iu1im9muAoGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(education)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCIfTqbCAtv-",
        "outputId": "4af9700c-4f73-498d-9aaa-070e80f5d6ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'EDUCATION Syracuse University', 'School', 'Syracuse University'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# load the English language model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "\n",
        "\n",
        "# process the resume text with SpaCy\n",
        "doc = nlp(data)\n",
        "\n",
        "# iterate over named entities in the document\n",
        "for ent in doc.ents:\n",
        "    # print the entity text and its label\n",
        "    print(ent.text, ent.label_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yG10eFKvraJE",
        "outputId": "696f55f0-f6ae-41f1-8fdf-e6866d11d429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rami-Lionel Kuttab ORG\n",
            "917 CARDINAL\n",
            "University, School of Engineering and Computer Science ORG\n",
            "Syracuse GPE\n",
            "NY ORG\n",
            "August 2022 DATE\n",
            "May 2024 DATE\n",
            "M.S. GPE\n",
            "Algorithms PERSON\n",
            "Structured Programming ORG\n",
            "School of Engineering and Computer Science ORG\n",
            "Syracuse GPE\n",
            "August 2018 DATE\n",
            "May 2022 DATE\n",
            "B.S. GPE\n",
            "Data Structures ORG\n",
            "Algorithms PERSON\n",
            "Introduction ORG\n",
            "A.I. GPE\n",
            "Operating Systems ORG\n",
            "Machine Learning ORG\n",
            "Physics ORG\n",
            "BGC Partners ORG\n",
            "New York GPE\n",
            "New York GPE\n",
            "June 2022 DATE\n",
            "August 2022 DATE\n",
            "25 % PERCENT\n",
            "three CARDINAL\n",
            "Pandas Library PERSON\n",
            "12% PERCENT\n",
            "July 2021 DATE\n",
            "August 2021 DATE\n",
            "WhatsApp ORG\n",
            "React Native ORG\n",
            "Utilized Firebase’s ORG\n",
            "Front-End Developer                                                                   ORG\n",
            "February 2021 – DATE\n",
            "June 2021 DATE\n",
            "HTML ORG\n",
            "CSS ORG\n",
            "JavaScript ORG\n",
            "daily DATE\n",
            "60 CARDINAL\n",
            "HTML ORG\n",
            "CSS ORG\n",
            "Bootstrap Led PERSON\n",
            "three CARDINAL\n",
            "Spotify Recommendation System ORG\n",
            "August 2022 DATE\n",
            "December 2022 DATE\n",
            "Used Collaborative Filtering WORK_OF_ART\n",
            "Spotify FAC\n",
            "Spotify Web API ORG\n",
            "30 CARDINAL\n",
            "the Web Spotify API FAC\n",
            "SKILLS Programming Languages: Python ORG\n",
            "SQL ORG\n",
            "Java PERSON\n",
            "Oracle, C ORG\n",
            "C++ GPE\n",
            "PHP ORG\n",
            "CSS ORG\n",
            "JavaScript ORG\n",
            "Ruby Database Management ORG\n",
            "SQL ORG\n",
            "PostgreSQL GPE\n",
            "SQLite Software ORG\n",
            "MS Excel ORG\n",
            "MS Word PERSON\n",
            "MS PowerPoint PERSON\n",
            "Visual Studios ORG\n",
            "National Social Service Scheme ORG\n",
            "Beit Hanina GPE\n",
            "Israel GPE\n",
            "Summer 2019 DATE\n",
            "15 CARDINAL\n",
            "Beit Hanina GPE\n",
            "zero CARDINAL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Matching"
      ],
      "metadata": {
        "id": "03Sw5i9D7nCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This code is an example of how to use natural language processing (NLP) techniques to match job skills in a job description to skills listed in a resume.\n",
        "\n",
        "The code loads the English language model for NLP using the spacy library and defines the job description and resume as strings. It then creates a PhraseMatcher object and adds a list of skills to it. The code uses the matcher to extract relevant features from the job description and resume, converts the features into a feature matrix using a CountVectorizer, and calculates the cosine similarity between the feature vectors. Finally, the code prints the cosine similarity score.\n",
        "\n",
        "Overall, this code can be used as a starting point for building an automated resume screening system that matches job skills to resumes, which can save time for HR professionals and recruiters."
      ],
      "metadata": {
        "id": "RqsWvt8oTq7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from spacy.matcher import PhraseMatcher\n",
        "\n",
        "# Load the English language model for NLP\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Define the resumes and job descriptions as strings\n",
        "resume_text = data\n",
        "job_text = description\n",
        "\n",
        "# Define the list of skills to match against\n",
        "#skills = ['Python', 'Java', 'software engineering']\n",
        "\n",
        "# Create a PhraseMatcher object and add the skills to it\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "for skill in skills:\n",
        "    matcher.add('Skill', None, nlp(skill))\n",
        "\n",
        "# Convert the text into spaCy documents\n",
        "resume_doc = nlp(resume_text)\n",
        "job_doc = nlp(job_text)\n",
        "\n",
        "# Extract the relevant features from the documents using the PhraseMatcher\n",
        "resume_skills = [resume_doc[start:end].text for match_id, start, end in matcher(resume_doc)]\n",
        "job_skills = [job_doc[start:end].text for match_id, start, end in matcher(job_doc)]\n",
        "\n",
        "# Combine the skills into a single text string for vectorization\n",
        "resume_skill_text = ' '.join(resume_skills)\n",
        "job_skill_text = ' '.join(job_skills)\n",
        "\n",
        "# Create a CountVectorizer to convert the skill text into a feature matrix\n",
        "vectorizer = CountVectorizer()\n",
        "try:\n",
        "    skill_matrix = vectorizer.fit_transform([resume_skill_text, job_skill_text])\n",
        "except ValueError:\n",
        "    print('No valid features found in the input data')\n",
        "    skill_matrix = None\n",
        "\n",
        "if skill_matrix is not None:\n",
        "    # Calculate the cosine similarity between the two feature vectors\n",
        "    cosine_sim = cosine_similarity(skill_matrix[0], skill_matrix[1])\n",
        "\n",
        "    # Print the cosine similarity score\n",
        "    print('Cosine similarity: %.2f' % cosine_sim)\n",
        "    result = cosine_sim\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1stZp_B2ILV",
        "outputId": "45062b6d-4b67-463f-d021-e890698ae98d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity: 0.27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "filename = 'Results.csv'\n",
        "\n",
        "# Check if the file exists\n",
        "try:\n",
        "    with open(filename, 'r') as file:\n",
        "        reader = csv.reader(file)\n",
        "        # If file already exists, we read the existing data\n",
        "        data = list(reader)\n",
        "except FileNotFoundError:\n",
        "    # If file doesn't exist, create an empty list for data\n",
        "    data = []\n",
        "\n",
        "# Ask user for new data\n",
        "new_resume = resume_name\n",
        "if skill_matrix == None:\n",
        "  new_value = \"not relevant resume\"\n",
        "else:\n",
        "  new_value = result[0][0]\n",
        "\n",
        "\n",
        "# Check if the resume name already exists in the file\n",
        "resume_names = [row[0] for row in data]\n",
        "if new_resume in resume_names:\n",
        "    print('Error: Resume name already exists in file')\n",
        "else:\n",
        "    # Add new data to the data list\n",
        "    data.append([new_resume, new_value])\n",
        "\n",
        "    # Write the data to the CSV file\n",
        "    with open(filename, 'w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerows(data)\n"
      ],
      "metadata": {
        "id": "CRw3Hidm8TI1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c8aaec9-05e3-4f0c-b0e4-1c79fa27f49c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Resume name already exists in file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from spacy.matcher import PhraseMatcher\n",
        "\n",
        "def get_similarity(data,job_,skills):\n",
        "  \n",
        "  # Load the English language model for NLP\n",
        "  nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "  # Define the resumes and job descriptions as strings\n",
        "  resume_text = data\n",
        "  job_text = job_\n",
        "\n",
        "  # Define the list of skills to match against\n",
        "  #skills = ['Python', 'Java', 'software engineering']\n",
        "\n",
        "  # Create a PhraseMatcher object and add the skills to it\n",
        "  matcher = PhraseMatcher(nlp.vocab)\n",
        "  for skill in skills:\n",
        "      matcher.add('Skill', None, nlp(skill))\n",
        "\n",
        "  # Convert the text into spaCy documents\n",
        "  resume_doc = nlp(resume_text)\n",
        "  job_doc = nlp(job_text)\n",
        "\n",
        "  # Extract the relevant features from the documents using the PhraseMatcher\n",
        "  resume_skills = [resume_doc[start:end].text for match_id, start, end in matcher(resume_doc)]\n",
        "  job_skills = [job_doc[start:end].text for match_id, start, end in matcher(job_doc)]\n",
        "\n",
        "  # Combine the skills into a single text string for vectorization\n",
        "  resume_skill_text = ' '.join(resume_skills)\n",
        "  job_skill_text = ' '.join(job_skills)\n",
        "\n",
        "  # Create a CountVectorizer to convert the skill text into a feature matrix\n",
        "  vectorizer = CountVectorizer()\n",
        "  try:\n",
        "      skill_matrix = vectorizer.fit_transform([resume_skill_text, job_skill_text])\n",
        "  except ValueError:\n",
        "      print('No valid features found in the input data')\n",
        "      skill_matrix = None\n",
        "\n",
        "  if skill_matrix is not None:\n",
        "      # Calculate the cosine similarity between the two feature vectors\n",
        "      cosine_sim = cosine_similarity(skill_matrix[0], skill_matrix[1])\n",
        "\n",
        "      # Print the cosine similarity score\n",
        "      print('Cosine similarity: %.2f' % cosine_sim)\n",
        "      result = cosine_sim\n",
        "  return result[0][0]"
      ],
      "metadata": {
        "id": "zYpPOhHaeeqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_result = []"
      ],
      "metadata": {
        "id": "J6wJVZn_DGjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')\n",
        "data = extract_text_from_doc(\"/content/Rami-Lionel-Kuttab-Resume_SuggestedEdits.docx\")\n",
        "print(data)\n",
        "name =  extract_name(data)\n",
        "print(name)\n",
        "mobile = extract_mobile_number(data)\n",
        "print(mobile)\n",
        "email = extract_email(data)\n",
        "print(email)\n",
        "skills = extract_skills(data)\n",
        "print(skills)\n",
        "education = extract_education(data)\n",
        "print(education)\n",
        "result = get_similarity(data,description,skills)\n",
        "print(get_similarity(data,description,skills))\n",
        "final_result.append((\"Rami-Lionel-Kuttab\",result))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wEFkveubDri",
        "outputId": "9cd1e7df-1130-43d1-a328-b5ac7025812d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /drive\n",
            "Rami-Lionel Kuttab (917) 9945360 • rlk_23@outlook.com • linkedin.com/in/rami-kuttab/ EDUCATION Syracuse University, School of Engineering and Computer Science, Syracuse, NY   August 2022 – May 2024       M.S. in Computer Science Relevant Coursework: Operating Systems, Algorithms, Data Science, Computer Architecture, Structured Programming, Natural Language Processing Syracuse University, School of Engineering and Computer Science, Syracuse, NY                                      August 2018 – May 2022                                                                                                                B.S. in Computer Science Relevant Coursework: Data Structures, Algorithms, Agile Development, Database, Object Oriented in C++, Introduction into A.I., Operating Systems, Machine Learning, and Physics WORK EXPERIENCE Software Developer, BGC Partners, New York, New York                     June 2022 – August 2022 Developed and thoroughly tested the code, continuously refining and rewriting it as necessary. Collaborated with other programmers involved in the project, ultimately optimizing the code for a 25 % reduction in running time by transitioning from series to parallel implementation. Researched, designed, and implemented three new software programs that added functionalities to the trading system. Using Python and Pandas Library I built software for my colleagues to assist them in figuring out when the versions have been updated by automatically sending out an email. Participated in +50 code reviews to improve code quality that increased performance by 12% which promoted professional development. PROJECTS WhatsApp Clone – Full-Stack Developer                         July 2021 – August 2021  Developed full-stack clone for WhatsApp that enables users to send text messages to each other in real time. Designed and implemented a smooth user interface using React Native components and styling, while ensuring the app’s compatibility with multiple devices. Utilized Firebase’s real-time database authentication services for back-end, including secure data storage and messaging services for real-time chat. College Events – Front-End Developer                                                                  February 2021 – June 2021  Built, assessed, and produced a website using HTML, CSS, and JavaScript to produce an event organizing website for college students that had an average daily visitor of 60 college students. Utilized UML diagrams to develop a user-friendly interface using HTML and CSS with Bootstrap Led a team of three college students and followed agile development scrums with colleagues to ensure timely completion and release of the project.  Spotify Recommendation System – Data Scientist                                                                   August 2022 – December 2022  Used Collaborative Filtering to recommend songs based on the overlap of songs in playlists in the dataset. Utilized Spotify Web API to fetch data from the Spotify music catalog and manage user’s playlists and saved music. Implemented my own playlist and compared it to Spotify Web API to get recommendations on the top 30 related songs from the Web Spotify API to the current playlist created. TECHNICAL SKILLS Programming Languages: Python, R, SQL, Java, HTML, Oracle, C, C++, PHP, CSS, JavaScript, Ruby Database Management: SQL, PostgreSQL, SQLite Software: MS Excel, MS Word, MS PowerPoint, Visual Studios LEADERSHIP EXPERIENCE Campaign Leader, National Social Service Scheme, Beit Hanina, Israel                                                   Summer 2019 Administered garbage segregation implementation program with 15 volunteers for a village to create awareness. Communicated with the trash company to collect solid garbage waste from the rural parts of Beit Hanina. Devised a wet waste management process to generate organic manure with zero cost of production.\n",
            "Rami\n",
            "9179945360\n",
            "rlk_23@outlook.com\n",
            "{'Java', 'SQL', 'Ruby', 'CSS', 'Word', 'Excel', 'Oracle', 'HTML', 'Data Science', 'PHP', 'React', 'Python', 'PostgreSQL', 'Machine Learning', 'JavaScript'}\n",
            "{'EDUCATION Syracuse University', 'School', 'Syracuse University'}\n",
            "Cosine similarity: 0.27\n",
            "Cosine similarity: 0.27\n",
            "0.2672612419124244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Further Analysis"
      ],
      "metadata": {
        "id": "lpeE97zk77Xk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = extract_text_from_doc(\"/content/CCC resume.docx\")\n",
        "print(data)\n",
        "name =  extract_name(data)\n",
        "print(name)\n",
        "mobile = extract_mobile_number(data)\n",
        "print(mobile)\n",
        "email = extract_email(data)\n",
        "print(email)\n",
        "skills = extract_skills(data)\n",
        "print(skills)\n",
        "education = extract_education(data)\n",
        "print(education)\n",
        "result = get_similarity(data,description,skills)\n",
        "print(get_similarity(data,description,skills))\n",
        "final_result.append((\"CCC\",result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWVwkXnKfY-A",
        "outputId": "5eed975f-96ad-413e-f10f-8ff9f0bb251f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chih-Chia, Chen cchen64@syr.edu ▪ Syracuse, New York ▪ (315)-777-3497 linkedin.com/in/ChihChia-Chen/  SUMMARY:    I’m looking for internship and full-time role opportunities in software engineering and security.   EDUCATION:  Syracuse University, College of Engineering and Computer Science Fall 2022  Bachelor’s in computer science  Syracuse University, College of Engineering and Computer Science Fall 2024  Master’s in computer science  SKILLS:  Programming: Python, JAVA, C/C++, SQL, Data Structures, Software Specification & Design Applications: Microsoft Visual Studio Code, Microsoft Visual Studio, Microsoft SQL Server Management Studio Languages: English (Bilingual), Mandarin (Bilingual), Japanese (elementary)    RELATED COURSEWORK: Object Oriented Programming C++ Software Specification & Design Database Design SQL Software Security Basic Hardware/software Security Data Structures (Java) Object Oriented Programming Python Algorithms Network and Web Security Agile Software Development ENGINEERING APPLICATIONS:  Software Specification & Design, OGTH Delivery App                                                                          Fall 2021 Collaborated with a group of randomly assigned classmates and came up with the specification and design of the food delivery app.  Participated in the making of CRC cards, software requirement documentation, use case description and diagram, class diagram, sequence diagrams, and state machine diagrams.  Personally responsible for organizing the fair distribution of workload and weekly meetings to review project progress Guitar Shop Database, Curriculum Project           Spring 2022  Designed the database based on a set of given guidelines and restrictions. Built the database from the ground up including an ER diagram specifying the relationships and other relevant information between each of the data tables. Implemented views, procedures, functions, scripts, and many more which enables the database to be interactive and efficient.  Basic Web Server, Curriculum Project                                                                                                    Fall 2022  Utilized existing library from Java such as Java.net, Java.util, and more to implement various functionality of the server.  Implemented support for the server to handle GET, HEAD, and POST request.  Built the security system surrounding the server including an authentication system as well as a self-signed certificate for SSL.   WORK EXPERIENCE:  11th Armory Squad Leader, 1st Company, 1st Squadron, 10th Army Command, ROCA, Taichung Oct 2020 – Feb 2021 Served mandatory military service as part of civic duty outlined in the constitution of ROC (Taiwan). Coordinated and led the squad’s movement during combat simulation. Organize squad’s scheduled task of armory ware maintenance. LEADERSHIP/ACTIVITIES:  Club Member, Information Security Club Jan 2019 – Fall 2021              Participated in club-hosted activities, challenges, and practices.              Participated in CNY Hackathon 2021 was tasked with completing CTF challenges.\n",
            "Chih\n",
            "3157773497\n",
            "cchen64@syr.edu\n",
            "{'Java', 'Python', 'SQL', 'JAVA'}\n",
            "{'Syracuse University'}\n",
            "Cosine similarity: 0.64\n",
            "Cosine similarity: 0.64\n",
            "0.6396021490668313\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = extract_text_from_scanned_pdf(\"/content/Sai sreeramNachireddi_resume.pdf\")\n",
        "print(data)\n",
        "name =  extract_name(data)\n",
        "print(name)\n",
        "mobile = extract_mobile_number(data)\n",
        "print(mobile)\n",
        "email = extract_email(data)\n",
        "print(email)\n",
        "skills = extract_skills(data)\n",
        "print(skills)\n",
        "education = extract_education(data)\n",
        "print(education)\n",
        "result = get_similarity(data,description,skills)\n",
        "print(get_similarity(data,description,skills))\n",
        "final_result.append((\"Sai\",result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35hxXEZlgOWF",
        "outputId": "9ab1cdec-23df-47b4-b2cb-541ea8f66c11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sai Sreeram Nachireddi 321 Westcott Street, Apt. 4, Syracuse, NY, 13210 | 680-697-4695 | ssrn.2021@gmail.com | https:/Mwww.linkedin.com/in/nachireddi-sai-sreeram-16414023a   EDUCATION Syracuse University - College of Engineering & Computer Science, Syracuse, NY August 2022 - May 2024 Master of Science in Computer Science Courses: Principles of Operating Systems, Design & Analysis of Algorithms, Natural Language Processing GPA: 3.44/4 Jawaharlal Nehru Technological University - Gokaraju Rangaraju Institute of Engineering & Technology, Hyderabad, India June 2018 - June 2022 Bachelor of Technology in Computer Science & Engineering Courses: Data Structures, Algorithms, Artificial Intelligence, Database Management Systems, Web Development GPA: 8.25/10 PROFESSIONAL EXPERIENCE Programmer Analyst Intern, Cognizant — Hyderabad, India January 2022 - June 2022 e Developed a search filtering feature using Java programming language and by deploying Spring Boot e Managed and collaborated closely with a team of five other members Web Development Intern, Sapphirus Systems Pvt Ltd — Hyderabad, India August 2021 - November 2021 e Designed frontend and UI of website with HTML, CSS and JavaScript e Collaborated with senior engineers to ensure compliance with Software Development Processes ACADEMIC PROJECTS Sentiment Analysis for Movie Reviews September 2022 — December 2022 e Performed various text classification tasks such as investigating data in order to choose pre-processing or filtering, tokenization, finding frequent words, use cross-validation to obtain precision, recall and F-measure scores, included various feature sets like bigrams, trigrams, POS tagging, subjectivity and sentiment lexicon e Calculated and compared accuracies at different stages by performing Naive Bayes classification, Logistic regression and decision tree classification e The sentiment label ranges from: “negative”, “somewhat negative”, “neutral”, “somewhat positive’, “positive” Automatic Monitoring of Helmet & Over speeding July 2021 - December 2021 e Emphasize on detecting fast-moving vehicles on road and if whether motorcyclists are wearing a helmet or not using the YOLO V3 Algorithm e Detecting speed of moving vehicles is done through OpenCV. The deep learning models are implemented using Keras e Incase of violation of traffic rules such as over speeding or absence of helmet usage, number plate of vehicle is detecting using deep convolutional neural networks (CNNs) Automatic Hand Sanitizer Dispenser January 2021 - July 2021 e Built a contact less sanitizer that dispenses sanitizer automatically as well as display nearby hospitals if the temperature of user exceeds 100-degree Fahrenheit for the user to get tested for Covid-19 e The model uses ESP 8266, Arduino UNO, ultrasonic sensor, DHT senor, water pump, hand sanitizer and 16*2 LCD display SKILLS e Languages: Java, Python, JavaScript, HTML 5, CSS 3, C e Frameworks: Spring Boot, React JS, jQuery, Node.js e Database: Microsoft SQL Server, Transact-SQL AWARDSICERTIFICATIONS e Data Science for Engineers, NPTEL (2021) e User-Centric Computing for Human-Computer Interaction, NPTEL (2021) e AWS Academy Cloud Foundations, AWS(2021) \f\n",
            "Sai\n",
            "6806974695\n",
            "ssrn.2021@gmail.com\n",
            "{'Java', 'SQL', 'CSS', 'HTML', 'AWS', 'Data Science', 'React', 'Python', 'JavaScript'}\n",
            "{'EDUCATION Syracuse University', 'AWS Academy Cloud Foundations', 'Jawaharlal Nehru Technological University'}\n",
            "Cosine similarity: 0.50\n",
            "Cosine similarity: 0.50\n",
            "0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = extract_text_from_scanned_pdf(\"/content/Resume-UD.pdf\")\n",
        "print(data)\n",
        "name =  extract_name(data)\n",
        "print(name)\n",
        "mobile = extract_mobile_number(data)\n",
        "print(mobile)\n",
        "email = extract_email(data)\n",
        "print(email)\n",
        "skills = extract_skills(data)\n",
        "print(skills)\n",
        "education = extract_education(data)\n",
        "print(education)\n",
        "result = get_similarity(data,description,skills)\n",
        "print(get_similarity(data,description,skills))\n",
        "final_result.append((\"UD\",result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_L-4983EkmSw",
        "outputId": "d392870e-e89e-4cd3-e64a-8c19a09a1a4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uday Kumar Putta 966 Lancaster Ave, Syracuse,NY | +1315 664 6584 | uputta@syr.edu EDUCATION Syracuse University - College of Engineering & Computer Science, Syracuse, NY August 2022 - May 2024 Master of Science in Cyber Security * Course Work: Cryptography, Design Analysis and Algorithm, Operating Systems Gandhi Institute of Technology and Management - School of technology, Hyderabad, July 2018 - May 2022 Telangana Bachelor of Technology , Computer Science, June 2022 * CGPA: 8.03/10 * Data Mining and Data Warehousing, Formal Languages and Automata Theory, Software Engineering , Cloud computing, Machine Learning, Cryptography and Network Security. EXPERIENCE cyber Security Intern, VERZEO, Hyderabad, Telangana June 2021 - September 2021 Performed Foot printing on Amazon website and registrant information IP Address using shodan, Netcraft, DNS dumpster. + Implemented SQL Injections on websites to access the data through credentials. * Utilized set tool in kali linux created fake gmail to capture data of the user. * Implemented DOS attack on Windows 7 virtual machine and accomplished to flood it with requests and prevent from usage. PROJECTS Mortality Prediction In ICU Using Artificial Neural Network January 2022 * Developed a model to predict mortality rate using known medical records. + Forecasted and reliable prediction for some sensitive medical illness or condition will help in reducing the mortality. + Proposed an mode! to reduce the time of implementation to provide the desired result. + Attained more than 90% accuracy in predicting the right medication and treatment for illness. Novel Stacked ConvNet for Malarial Parasite Detection August 2021 * Classifying single cell images of dataset into thin blood smears to detect Plasmodium parasite using CNN, Deep learning . * Trained and tested by using a dataset from National library of Medicine Lister Hill National Center for Bio medical Communications contains 21000+ Images of blood cells taken from a patient. * Proposed Convnet architecture to classify and evaluate the cell images and provide positive or negative results as programmed. + Performed task using adam optimizer to speculate outcome of processed cell images and achieved 95% + accuracy. SKILLS + Programming Languages: C/C++, Python, Java + Web Development: HTMLS5, CSS * Tools: Jupyter , Eclipse, Unity, Visual Studio , Windows Ubuntu, Kali Linux * Managerial : Leadership, Communication, Time Management, Problem solving, Active Listening, Multi Tasking CO-CURRICULAR ACTIVITIES * Ethical Hacking Workshop - by Elan Nvision * NASA Galactic- International Space Apps Challenge Pre- qualifiers and Qualifiers + 3D Game development Workshop By IIT Guwahati * Cyber Forensics Workshop By IIT Roorkee CERTIFICATIONS * Cryptography , University of Maryland + Initiating and Planning Projects , University of California + Software Architecture, University of Alberta * Integrated Development Environments in Linux   \f\n",
            "Uday\n",
            "+13156646584\n",
            "uputta@syr.edu\n",
            "{'Java', 'SQL', 'CSS', 'Software Architecture', 'Python', 'Machine Learning'}\n",
            "{'EDUCATION Syracuse University', 'University', 'School of technology'}\n",
            "Cosine similarity: 0.35\n",
            "Cosine similarity: 0.35\n",
            "0.35355339059327373\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = extract_text_from_scanned_pdf(\"/content/Rajashekar_Resume.pdf\")\n",
        "print(data)\n",
        "name =  extract_name(data)\n",
        "print(name)\n",
        "mobile = extract_mobile_number(data)\n",
        "print(mobile)\n",
        "email = extract_email(data)\n",
        "print(email)\n",
        "skills = extract_skills(data)\n",
        "print(skills)\n",
        "education = extract_education(data)\n",
        "print(education)\n",
        "print(get_similarity(data,description,skills))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5lpATpIBXvh",
        "outputId": "6c692277-b4e8-4cb2-bceb-54a3deaad3ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAJASHEKAR REDDY MORA (315) 454-1924 * ramora@syr.edu * www.linkedin.com/in/rajashekar-reddy-mora   EDUCATION   Syracuse University, School of Information Studies, Syracuse, NY May 2024 M.S. Applied Data Science Relevant Coursework: Natural Language Processing | Data Administration and Data management | Introduction to Data Science | Database Management | Communication for Information Management Professionals National Institute of Technology | Calicut, India May 2019 B.Tech. Electrical and Electronics Engineering Relevant Coursework: Data Structures and Algorithms, Computer programming, Image Processing WORK EXPERIENCE   Applications Systems Coordinator, M.H.Alshaya Co. , Bangalore, India July 2019 — Jun 2022 e Extracted and analyzed retail data sources using SQL and Python to aid the Brand Marketing team with insights to drive informed decision making e Working on a sentiment analysis project aimed at automatically gauging the nature of the feedback provided to the IT help desk. Employed NLP methodologies using NLTK, scikit-learn, and pandas packages in Python. This would result in a significant reduction in the operational time from 3 days to 4 hours by automating the time-intensive manual process of labeling thousands of feedback messages e Conducted Statistical Evaluation on promotional activity effectiveness on individual brands and recommended the reconstruction of promotional strategies based on brands’ affinity towards various marketing channels e Supported client team in tracking the brand performance by regular reporting and worked on numerous ad-hoc analytical requests which helped in formulating the client’s business strategy PROJECTS   Farm Knights- Machine Learning Jan 2019 — Apr 2019 e Built a Machine learning application, with three different modules each on regression, image classification and object detection. First, to predict the farm produce in the next 6 months using historical pricing data using regression techniques. Second, an image classification module to identify crop diseases based on patterns on leaves using CNNs. Last one is an Object detection module to separate weeds from crops with a bounding box around them e With the help Support Vector Regression implementation of LinearSVR using sklearn library we have performed the prediction task with an accuracy of more than 90% e Object detection task has been performed using Tensorflow Faster R CNN Object Detection model API, collected images from Google image search and labelled them using Labellmg. e Using Flask framework, HTML and CSS, we have developed a python based backend to serve these models on a web based frontend Information System Analysis - Student Advising System Jan 2019 — Apr 2019 e Collaborated with key stakeholders to gather functional, non-functional requirements to build student advising system e Designed the user-friendly workflows for all identified use cases using the user experience design principles e Designed the database model, data flows and the working prototype for the application and presented to the college e Influenced the college dropout rates by 10% drop by acting on the customer insights received on student issues TECHNICAL SKILLS/CERTIFICATIONS   Programming Languages: Python, CPP,C, Competitive programming, R, SQL, HTML, Oracle Database Management: SQL Software: Tensorflow, Pytorch,Flask,HTML,CSS,MS Excel, MS Word, MS PowerPoint LEADERSHIP EXPERIENCE   e Treasurer for National Cadet Corps (NCC), NIT Calicut chapter e Assistant Secretary for Forum of Dance and Dramatics, NIT Calicut e Senior Executive for Tathva (Tech fest), NIT Calicut \f\n",
            "RAJASHEKAR\n",
            "3154541924\n",
            "ramora@syr.edu\n",
            "{'SQL', 'python', 'Machine learning', 'CSS', 'Word', 'Excel', 'Oracle', 'Flask', 'HTML', 'Data Science', 'Python', 'Machine Learning'}\n",
            "{'EDUCATION Syracuse University', 'School', 'Information Management Professionals National Institute'}\n",
            "Cosine similarity: 0.36\n",
            "0.3621429841700741\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = extract_text_from_doc(\"/content/John Doe.docx\")\n",
        "print(data)\n",
        "name =  extract_name(data)\n",
        "print(name)\n",
        "mobile = extract_mobile_number(data)\n",
        "print(mobile)\n",
        "email = extract_email(data)\n",
        "print(email)\n",
        "skills = extract_skills(data)\n",
        "print(skills)\n",
        "education = extract_education(data)\n",
        "print(education)\n",
        "result = get_similarity(data,description,skills)\n",
        "print(get_similarity(data,description,skills))\n",
        "final_result.append((\"JohnDoe\",result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rr4s2e7sBw1e",
        "outputId": "98e1da6f-1af8-4518-b191-9c68e6454116"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "John Doe 123 Main St. | Anytown, USA 12345 | john.doe@email.com | (123) 456-7890 Objective: Software engineer with over 5 years of experience seeking a challenging position in a dynamic company that values innovation and excellence. Experience: Software Engineer ABC Company | Anytown, USA | 2018 - Present - Designed and developed custom software solutions for clients using Python and Java - Worked collaboratively with project managers and clients to define project requirements and deliverables - Mentored junior engineers and provided technical leadership to project teams Software Engineer XYZ Company | Anytown, USA | 2015 - 2018 - Designed and developed software applications for internal use using C++ and Python - Collaborated with cross-functional teams to ensure software met business requirements - Implemented unit and integration testing to ensure high-quality software Education: Bachelor of Science in Computer Science Anytown University | Anytown, USA | 2015 Skills: - Proficient in Python, Java, C++, and SQL - Experience with software design patterns and best practices - Strong problem-solving and analytical skills - Excellent communication and collaboration skills - Familiarity with agile development methodologies\n",
            "John\n",
            "4567890\n",
            "john.doe@email.com\n",
            "{'Java', 'Python', 'SQL'}\n",
            "{'Computer Science Anytown University'}\n",
            "Cosine similarity: 0.53\n",
            "Cosine similarity: 0.53\n",
            "0.5345224838248488\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(final_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPrm-OuEDDnB",
        "outputId": "278f12be-e25d-4a2f-e685-89b78fa4d190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Rami-Lionel-Kuttab', 0.2672612419124244), ('CCC', 0.6396021490668313), ('Sai', 0.5), ('UD', 0.35355339059327373), ('JohnDoe', 0.5345224838248488)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('Final_result.csv', mode=\"w\", newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Resume-Name', 'Similarity'])\n",
        "    for row in final_result:\n",
        "        writer.writerow(row)\n",
        "        print(file.closed) # Check if file is still open\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Evd50M8HFfti",
        "outputId": "fa02b8f0-d426-4030-be4d-741055a7aa0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n"
          ]
        }
      ]
    }
  ]
}